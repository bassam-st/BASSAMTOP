# bassam_core/workers/core_worker.py
# -*- coding: utf-8 -*-
"""
ğŸ”¥ Bassam Core Worker â€“ FINAL
- ØµÙÙ‘ Ù…Ù‡Ø§Ù… Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (enqueue â†’ drain).
- ØªØ¹Ù„Ù‘Ù… Ø°Ø§ØªÙŠ Ù…Ø¬Ø¯ÙˆÙ„ (Scheduler) + ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¹Ø¨Ø± API.
- ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨ØµÙŠØºØ© JSONL Ø¯Ø§Ø®Ù„ bassam_core/data.
- Ø¨Ø­Ø« Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¹Ø¨Ø± DuckDuckGo (ÙŠÙ…ÙƒÙ† Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¥Ø¶Ø§ÙØ© Google).
"""

from __future__ import annotations

import os
import json
import time
import threading
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from duckduckgo_search import DDGS  # Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ

# =========================
# Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„ØªØ®Ø²ÙŠÙ†
# =========================
PKG_DIR  = os.path.dirname(os.path.dirname(__file__))       # bassam_core/
DATA_DIR = os.path.join(PKG_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

NEWS_PATH = os.path.join(DATA_DIR, "news.jsonl")        # Ø£Ø±Ø´ÙŠÙ Ù…Ø§ ØªØ¹Ù„Ù‘Ù…Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…
QUEUE_PATH = os.path.join(DATA_DIR, "queue.jsonl")      # ØµÙÙ‘ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
KNOW_PATH  = os.path.join(DATA_DIR, "knowledge.jsonl")  # Ù…Ø¹Ø±ÙØ© ØªØ±Ø§ÙƒÙ…ÙŠØ© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)

# =========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø©
# =========================
INTERVAL_MIN = int(os.getenv("LEARN_INTERVAL_MIN", "30"))
INTERVAL_SEC = int(os.getenv("LEARN_INTERVAL_SEC", "0"))
RUN_IMMEDIATELY = os.getenv("LEARN_RUN_IMMEDIATELY", "1") == "1"

# =========================
# Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„ØªØ¹Ù„Ù‘Ù… Ø§Ù„Ø°Ø§ØªÙŠ
# =========================
TOPICS: List[str] = [
    "ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© 2025",
    "Ø£Ø·Ø± Python Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬ÙŠØ©: FastAPI Ùˆ Flask",
    "ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AsyncIO",
    "ØªØµÙ…ÙŠÙ… Chatbots ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ RAG Ùˆ LLMs",
    "Ø£ÙØ¶Ù„ Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„ØªØ´ÙÙŠØ± ÙˆØ§Ù„Ø£Ù…Ø§Ù† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨",
    "Ø§Ø³ØªØ®Ø¯Ø§Ù… APIs Ù…ÙØªÙˆØ­Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰",
    "ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„Ø¹Ù…Ù„",
]

# =========================
# Ø£Ø¯ÙˆØ§Øª JSONL Ø¨Ø³ÙŠØ·Ø©
# =========================
def _append_jsonl(path: str, obj: Dict[str, Any]) -> None:
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _read_jsonl(path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()
    if limit:
        lines = lines[-limit:]
    return [json.loads(x) for x in lines]

# =========================
# ØµÙÙ‘ Ø§Ù„Ù…Ù‡Ø§Ù… ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬
# =========================
_queue_lock = threading.Lock()
_queue: List[Dict[str, Any]] = []

_running = threading.Event()      # Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ø§Ù…Ù„
_NEXT_AT: Optional[str] = None    # ÙˆÙ‚Øª Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© (UTC ISO)

def enqueue_task(q: str) -> None:
    """Ø¥Ø¶Ø§ÙØ© Ù…Ù‡Ù…Ø© (Ù†ØµÙ‘ Ø¨Ø­Ø«/ØªØ¹Ù„Ù‘Ù…) Ø¥Ù„Ù‰ Ø§Ù„ØµÙÙ‘."""
    q = (q or "").strip()
    if not q:
        return
    item = {"q": q, "ts": datetime.utcnow().isoformat()}
    with _queue_lock:
        _queue.append(item)
    _append_jsonl(QUEUE_PATH, item)

def query_index() -> List[str]:
    """Ø¹Ø±Ø¶ Ø¢Ø®Ø± Ø¹Ø¨Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØµÙ (Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ ÙÙ‚Ø·)."""
    with _queue_lock:
        return [x["q"] for x in _queue[-15:]]

def get_status() -> Dict[str, Any]:
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ù…Ø¬Ø¯ÙˆÙ„ (ØªØ³ØªØ¯Ø¹ÙŠÙ‡Ø§ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©)."""
    return {
        "running": _running.is_set(),
        "interval_min": INTERVAL_MIN,
        "interval_sec": INTERVAL_SEC,
        "queue_size": len(_queue),
        "topics": TOPICS,
        "next_run_at": _NEXT_AT,  # ÙˆÙ‚Øª Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© (UTC ISO)
    }

def get_latest_results(limit: int = 10) -> List[Dict[str, Any]]:
    """Ø¢Ø®Ø± Ù…Ø§ ØªÙ…Ù‘ ØªØ¹Ù„Ù…Ù‡/Ø£Ø±Ø´ÙØªÙ‡ (Ù„Ù„Ù€ /api/news /api/learn/latest)."""
    docs = _read_jsonl(NEWS_PATH, limit=limit)
    return list(reversed(docs))

# =========================
# Ø§Ù„Ø¨Ø­Ø« + Ø§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø¨Ø³ÙŠØ·
# =========================
def _ddg_search(q: str, n: int = 8) -> List[Dict[str, str]]:
    """Ø¨Ø­Ø« Ø¹Ø¨Ø± DuckDuckGo ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø³Ù‘Ø·Ø©."""
    results: List[Dict[str, str]] = []
    try:
        with DDGS() as d:
            for r in d.text(q, max_results=n, region="sa-ar"):
                results.append({
                    "title": r.get("title", "") or "",
                    "url": r.get("href", "") or "",
                    "snippet": (r.get("body") or "")[:300],
                })
    except Exception as e:
        results.append({"title": "SearchError", "url": "", "snippet": str(e)})
    return results

def _summarize(q: str, results: List[Dict[str, str]]) -> str:
    """ØªÙ„Ø®ÙŠØµ Ù†ØµÙŠ Ù…Ø¨Ø³Ù‘Ø· Ù„Ø£ÙˆÙ‘Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Ù…ÙƒØ§Ù† Ù…Ù†Ø§Ø³Ø¨ Ù„Ø§Ø­Ù‚Ù‹Ø§ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ LLM)."""
    lines = [f"- {r['title']}: {r['snippet']}" for r in results[:5] if r.get("title") or r.get("snippet")]
    if not lines:
        lines = ["- Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙƒØ§ÙÙŠØ©."]
    return f"ğŸ“˜ Ù…Ù„Ø®Øµ Ø­ÙˆÙ„ Â«{q}Â»:\n" + "\n".join(lines)

def _learn_once(q: str) -> Dict[str, Any]:
    """ÙŠÙ†ÙÙ‘Ø° Ø¨Ø­Ø«Ù‹Ø§ + ØªÙ„Ø®ÙŠØµÙ‹Ø§ ÙˆÙŠØ®Ø²Ù† Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ø§Ù„Ø£Ø±Ø´ÙŠÙ."""
    results = _ddg_search(q)
    summary = _summarize(q, results)
    doc = {
        "query": q,
        "summary": summary,
        "timestamp": datetime.utcnow().isoformat(),
        "results": results,
    }
    _append_jsonl(NEWS_PATH, doc)
    _append_jsonl(KNOW_PATH, doc)
    return doc

def _drain_queue() -> int:
    """ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØµÙÙ‘ (Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø©)."""
    global _queue
    with _queue_lock:
        if not _queue:
            return 0
        batch = _queue[:]
        _queue = []
    count = 0
    for item in batch:
        try:
            _learn_once(item["q"])
            count += 1
        except Exception as e:
            _append_jsonl(NEWS_PATH, {"error": str(e), "query": item["q"], "ts": datetime.utcnow().isoformat()})
    return count

# =========================
# Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ Scheduler
# =========================
class Scheduler:
    def __init__(self, minutes: int = INTERVAL_MIN, seconds: int = INTERVAL_SEC, run_now: bool = RUN_IMMEDIATELY):
        self.interval = max(1, minutes * 60 + seconds)
        self.run_now = run_now
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._loop, daemon=True)

    def start(self):
        global _NEXT_AT
        # Ø­Ø³Ø§Ø¨ ÙˆÙ‚Øª Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø© ÙÙˆØ± Ø§Ù„Ø¨Ø¯Ø¡
        _NEXT_AT = (datetime.utcnow() + timedelta(seconds=self.interval)).isoformat()
        print(f"ğŸ•’ Scheduler started (every {self.interval//60} min {self.interval%60}s)")
        self.thread.start()

    def stop(self):
        self.stop_event.set()
        print("ğŸ›‘ Scheduler stopped")

    def _sleep_chunked(self):
        # Ù†ÙˆÙ… Ù…Ø¬Ø²Ø£ ÙŠØ³Ù…Ø­ Ø¨Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø³Ù„Ø³
        remaining = self.interval
        step = 0.1
        loops = int(remaining / step)
        for _ in range(loops):
            if self.stop_event.is_set():
                return
            time.sleep(step)

    def _loop(self):
        if self.run_now:
            self._tick()
        while not self.stop_event.is_set():
            self._sleep_chunked()
            if self.stop_event.is_set():
                break
            self._tick()

    def _tick(self):
        global _NEXT_AT
        try:
            run_cycle_once()  # ØªÙ†ÙÙŠØ° Ø¯ÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø© (ØµÙ + Ù…ÙˆØ§Ø¶ÙŠØ¹)
        finally:
            _NEXT_AT = (datetime.utcnow() + timedelta(seconds=self.interval)).isoformat()

# =========================
# ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ / Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„
# =========================
def run_cycle_once(custom_topics: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¹Ù„Ù‘Ù… ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ø¢Ù†:
      1) ÙŠÙØ±Ù‘Øº Ø§Ù„ØµÙÙ‘ (Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…).
      2) ÙŠØªØ¹Ù„Ù‘Ù… Ù…Ù† Ù‚Ø§Ø¦Ù…Ø© Ù…ÙˆØ§Ø¶ÙŠØ¹ (Ù…Ø®ØµÙ‘ØµØ© Ø£Ùˆ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©).
    """
    print(f"ğŸ” Auto-learning cycle @ {datetime.utcnow().isoformat()}")

    done_from_queue = _drain_queue()
    topics = [t for t in (custom_topics or TOPICS) if t and t.strip()]

    done_from_topics = 0
    for topic in topics:
        try:
            _learn_once(topic.strip())
            done_from_topics += 1
        except Exception as e:
            _append_jsonl(NEWS_PATH, {"topic": topic, "error": str(e), "ts": datetime.utcnow().isoformat()})

    msg = f"âœ… Cycle complete â€” queue:{done_from_queue}, topics:{done_from_topics}"
    print(msg)
    return {"queue": done_from_queue, "topics": done_from_topics, "message": msg}

# =========================
# ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø¯Ø¡ Ø§Ù„Ø¹Ø§Ù…Ù„ Ù…Ù† ØªØ·Ø¨ÙŠÙ‚ FastAPI
# =========================
_SCHED: Optional[Scheduler] = None

def start_scheduler() -> None:
    """ØªÙØ³ØªØ¯Ø¹Ù‰ Ù…Ù† Ø­Ø¯Ø« startup ÙÙŠ FastAPI Ù„Ø±Ø¨Ø· Ø§Ù„Ø¹Ø§Ù…Ù„ Ø¨Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„."""
    global _SCHED
    if _SCHED:
        return
    _running.set()
    _SCHED = Scheduler()
    _SCHED.start()
    print("ğŸ”¥ Worker linked to Scheduler and running.")
