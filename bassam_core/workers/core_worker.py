# bassam_core/workers/core_worker.py
# -*- coding: utf-8 -*-
"""
ðŸ”¥ Bassam Core Worker â€“ FINAL
- Ø±Ø¨Ø· Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ Ø¨Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§.
- ØªÙ†ÙÙŠØ° Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (ØµÙ Ù…Ù‡Ø§Ù…).
- ØªØ¹Ù„Ù‘Ù… Ø°Ø§ØªÙŠ Ù…Ø¬Ø¯ÙˆÙ„ + Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¹Ù„Ù‘Ù… ÙŠØ¯ÙˆÙŠØ§Ù‹ Ø¹Ø¨Ø± API.
"""

import os
import json
import time
import threading
from datetime import datetime
from typing import List, Dict, Any, Optional
from duckduckgo_search import DDGS

# ==== Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØªØ®Ø²ÙŠÙ† ====
ROOT_DIR = os.path.dirname(os.path.dirname(__file__))
DATA_DIR = os.path.join(ROOT_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

NEWS_PATH = os.path.join(DATA_DIR, "news.jsonl")       # Ø£Ø±Ø´ÙŠÙ Ø§Ù„ØªØ¹Ù„Ù‘Ù…
QUEUE_PATH = os.path.join(DATA_DIR, "queue.jsonl")     # ØµÙÙ‘ Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
KNOW_PATH  = os.path.join(DATA_DIR, "knowledge.jsonl") # Ù…Ø¹Ø±ÙØ© ØªØ±Ø§ÙƒÙ…ÙŠØ©

# ==== Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© ====
INTERVAL_MIN = int(os.getenv("LEARN_INTERVAL_MIN", "30"))
INTERVAL_SEC = int(os.getenv("LEARN_INTERVAL_SEC", "0"))
RUN_IMMEDIATELY = os.getenv("LEARN_RUN_IMMEDIATELY", "1") == "1"

# ==== Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ====
TOPICS = [
    "ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«Ø© 2025",
    "Ø£ÙØ·Ø± Python Ù„Ø¨Ù†Ø§Ø¡ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬ÙŠØ© FastAPI Ùˆ Flask",
    "ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AsyncIO",
    "ØªØµÙ…ÙŠÙ… Chatbots ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ RAG Ùˆ LLMs",
    "Ø£ÙØ¶Ù„ Ù…Ù…Ø§Ø±Ø³Ø§Øª Ø§Ù„ØªØ´ÙÙŠØ± ÙˆØ§Ù„Ø£Ù…Ø§Ù† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„ÙˆÙŠØ¨",
    "Ø§Ø³ØªØ®Ø¯Ø§Ù… APIs Ù…ÙØªÙˆØ­Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰",
    "ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„Ø¹Ù…Ù„",
]

# ==== Ø£Ø¯ÙˆØ§Øª JSONL ====
def _append_jsonl(path: str, obj: Dict[str, Any]) -> None:
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _read_jsonl(path: str, limit: Optional[int] = None) -> List[Dict[str, Any]]:
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        lines = f.read().splitlines()
    if limit:
        lines = lines[-limit:]
    return [json.loads(x) for x in lines]

# ==== ØµÙÙ‘ Ø§Ù„Ù…Ù‡Ø§Ù… ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬ ====
_queue_lock = threading.Lock()
_queue: List[Dict[str, Any]] = []

_running = threading.Event()

def enqueue_task(q: str) -> None:
    """Ø¥Ø¶Ø§ÙØ© Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¨Ø­Ø«/ØªØ¹Ù„Ù‘Ù…)."""
    item = {"q": q.strip(), "ts": datetime.utcnow().isoformat()}
    if not item["q"]:
        return
    with _queue_lock:
        _queue.append(item)
    _append_jsonl(QUEUE_PATH, item)

def query_index() -> List[str]:
    """Ø¢Ø®Ø± Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙÙŠ Ø§Ù„ØµÙÙ‘ (Ù„Ù„Ø¹Ø±Ø¶)."""
    with _queue_lock:
        return [x["q"] for x in _queue[-15:]]

def get_status() -> Dict[str, Any]:
    """Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ù…Ø¬Ø¯ÙˆÙ„."""
    return {
        "running": _running.is_set(),
        "interval_min": INTERVAL_MIN,
        "interval_sec": INTERVAL_SEC,
        "queue_size": len(_queue),
        "topics": TOPICS,
    }

def get_latest_results(limit: int = 10) -> List[Dict[str, Any]]:
    """Ø£Ø­Ø¯Ø« Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ¹Ù„Ù‘Ù…."""
    docs = _read_jsonl(NEWS_PATH, limit=limit)
    return list(reversed(docs))

# ==== Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ ====
def _ddg_search(q: str, n: int = 6) -> List[Dict[str, str]]:
    results = []
    try:
        with DDGS() as d:
            for r in d.text(q, max_results=n, region="sa-ar"):
                results.append({
                    "title": r.get("title", ""),
                    "href": r.get("href", ""),
                    "snippet": (r.get("body") or "")[:250],
                })
    except Exception as e:
        results.append({"title": "SearchError", "href": "", "snippet": str(e)})
    return results

def _summarize(q: str, results: List[Dict[str, str]]) -> str:
    lines = [f"- {r['title']}: {r['snippet']}" for r in results[:5]]
    if not lines:
        lines = ["- Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬."]
    return f"ðŸ“˜ Ù…Ù„Ø®Øµ Ø­ÙˆÙ„ Â«{q}Â»:\n" + "\n".join(lines)

def _learn_once(q: str) -> Dict[str, Any]:
    """Ø¨Ø­Ø« + ØªÙ„Ø®ÙŠØµ + ØªØ®Ø²ÙŠÙ† Ù„Ø¹Ù†ØµØ± ÙˆØ§Ø­Ø¯."""
    results = _ddg_search(q)
    summary = _summarize(q, results)
    doc = {
        "query": q,
        "summary": summary,
        "timestamp": datetime.utcnow().isoformat(),
        "results": results,
    }
    _append_jsonl(NEWS_PATH, doc)
    _append_jsonl(KNOW_PATH, doc)
    return doc

def _drain_queue() -> int:
    """ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ØµÙÙ‘."""
    count = 0
    global _queue
    with _queue_lock:
        if not _queue:
            return 0
        batch = _queue[:]
        _queue = []
    for item in batch:
        _learn_once(item["q"])
        count += 1
    return count

# ==== Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ ====
class Scheduler:
    def __init__(self, minutes=INTERVAL_MIN, seconds=INTERVAL_SEC, run_now=RUN_IMMEDIATELY):
        self.interval = minutes * 60 + seconds
        self.run_now = run_now
        self.stop_event = threading.Event()
        self.thread = threading.Thread(target=self._loop, daemon=True)

    def start(self):
        print(f"ðŸ•’ Scheduler started (every {self.interval//60} min)")
        self.thread.start()

    def stop(self):
        self.stop_event.set()
        print("ðŸ›‘ Scheduler stopped")

    def _sleep(self):
        for _ in range(int(self.interval * 10)):
            if self.stop_event.is_set():
                return
            time.sleep(0.1)

    def _loop(self):
        if self.run_now:
            self._tick()
        while not self.stop_event.is_set():
            self._sleep()
            if self.stop_event.is_set():
                break
            self._tick()

    def _tick(self):
        run_cycle_once()  # ØªÙ†ÙÙŠØ° Ø¯ÙˆØ±Ø© ÙƒØ§Ù…Ù„Ø©

# ==== ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ (Ù„Ù„Ù€ API) ====
def run_cycle_once(custom_topics: Optional[List[str]] = None) -> Dict[str, Any]:
    """ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØ¹Ù„Ù‘Ù… ÙƒØ§Ù…Ù„Ø© Ø§Ù„Ø¢Ù† (ÙŠÙ†Ø§Ø¯Ù‰ Ù…Ù† Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ Ø£Ùˆ Ù…Ù† API)."""
    print(f"ðŸ” Auto-learning cycle @ {datetime.utcnow().isoformat()}")
    done_from_queue = _drain_queue()
    topics = custom_topics if (custom_topics and len(custom_topics) > 0) else TOPICS
    done_from_topics = 0
    for topic in topics:
        try:
            _learn_once(topic)
            done_from_topics += 1
        except Exception as e:
            _append_jsonl(NEWS_PATH, {"topic": topic, "error": str(e), "ts": datetime.utcnow().isoformat()})
    msg = f"âœ… Cycle complete â€” queue:{done_from_queue}, topics:{done_from_topics}"
    print(msg)
    return {"queue": done_from_queue, "topics": done_from_topics, "message": msg}

# ==== Ø±Ø¨Ø· Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„ Ø¨Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„ ====
_SCHED: Optional[Scheduler] = None

def start_scheduler() -> None:
    """ÙŠØ³ØªØ¯Ø¹Ù‰ Ù…Ù† Ø­Ø¯Ø« Ø¨Ø¯Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚."""
    global _SCHED
    if _SCHED:
        return
    _running.set()
    _SCHED = Scheduler()
    _SCHED.start()
    print("ðŸ”¥ Worker linked to Scheduler and running.")
# Ø¯Ø§Ø®Ù„ bassam_core/workers/core_worker.py
from typing import List, Dict, Optional
from .search_providers import search_google, search_ddg
import os, time
from ..app.db import save_docs  # ØªØ£ÙƒØ¯ Ø£Ù† db.py ÙŠÙˆÙÙ‘Ø± save_docs

def do_search(q: str, source: str = "auto", max_results: int = 8) -> List[Dict]:
    """ÙŠØ¨Ø­Ø« Ø£ÙˆÙ„Ø§Ù‹ ÙÙŠ Google (Ø¥Ù† ØªÙˆÙÙ‘Ø±Øª Ø§Ù„Ù…ÙØ§ØªÙŠØ­) Ø«Ù… ÙŠØ³Ù‚Ø· Ø¹Ù„Ù‰ DDG."""
    source = (source or "auto").lower()
    results: List[Dict] = []
    if source in ("google", "auto"):
        try:
            results = search_google(q, max_results=max_results)
        except Exception:
            results = []
    if not results and source in ("ddg", "auto", "both"):
        results = search_ddg(q, max_results=max_results)
    if source == "both":  # Ø¯Ù…Ø¬ Ø§Ù„Ø¥Ø«Ù†ÙŠÙ†
        try:
            g = search_google(q, max_results=max_results//2 or 4)
        except Exception:
            g = []
        d = search_ddg(q, max_results=max_results - len(g))
        results = (g or []) + (d or [])
    return results

def learn_from_query(q: str, source: str = "auto") -> Dict:
    """ÙŠÙ†ÙÙ‘Ø° Ø¨Ø­Ø«Ù‹Ø§ ÙˆÙŠØ­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©."""
    docs = do_search(q, source=source, max_results=10)
    if docs:
        save_docs(docs)
    return {"learned": len(docs), "docs": docs[:5]}  # Ù†Ø±Ø¬Ù‘Ø¹ Ø¹ÙŠÙ‘Ù†Ø© ØµØºÙŠØ±Ø© Ù„Ù„Ø¹Ø±Ø¶

# Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„ÙƒÙ† Ù†Ø¶ÙŠÙ optional topics:
def run_cycle_once(topics: Optional[List[str]] = None) -> Dict:
    topics = topics or os.getenv("TOPICS", "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ, Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ").split(",")
    total = 0
    for t in [x.strip() for x in topics if x.strip()]:
        total += learn_from_query(t).get("learned", 0)
        time.sleep(1)
    return {"message": "cycle_done", "topics": len(topics), "learned": total}
